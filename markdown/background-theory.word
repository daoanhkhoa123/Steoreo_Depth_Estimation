<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>
<p><img src="image-7.png" /></p>
<h1 id="ii.-mathematical-background">II. Mathematical background</h1>
<h2 id="epipolar-geometry">1. Epipolar Geometry</h2>
<p>Epipolar geometry is the geometry of stereo vision. When two cameras
view a 3D scene from two distinct positions, there are a number of
geometric relations between the 3D points and their projections onto the
2D images that lead to constraints between the image points. These
relations are derived based on the assumption that the cameras can be
approximated by the pinhole camera model.</p>
<p><img src="image-8.png" /></p>
<h3 id="definition">1.1. Definition</h3>
<p>The figure below depicts two pinhole cameras looking at point X. In
real cameras, the image plane is actually behind the focal center.
However, we simplified the problem by placing a <em>virtual</em> image
plane in front of the camera, since real plane is symmetric about the
focal center of the lens.</p>
<p>Each camera captures a 2D image of the 3D world. This conversion from
3D to 2D is referred to as a perspective projection and is described by
the pinhole camera model.</p>
<p><img src="image-11.png" /></p>
<h3 id="epipole-or-epipolar-point">1.2. Epipole or epipolar point</h3>
<p>Each center projects onto a distinct point into the other camera’s
image plane. These two image points, denoted by <span
class="math inline"><em>e</em><sub><em>L</em></sub></span> and <span
class="math inline"><em>e</em><sub><em>R</em></sub></span>, are called
epipoles or epipolar points.</p>
<p>Ep`ipolar point is the intersection of line connecting two camera’s
center <span class="math inline"><em>O</em><sub><em>L</em></sub></span>
and <span class="math inline"><em>O</em><sub><em>R</em></sub></span>
with their image plane.</p>
<h3 id="epipolar-line">1.3. Epipolar line</h3>
<p>Line <span
class="math inline"><em>O</em><sub><em>L</em></sub> − <em>X</em></span>
projected on right camera creating a line <span
class="math inline"><em>e</em><sub><em>R</em></sub> − <em>X</em><sub><em>R</em></sub></span>
called the epipolar line. Symmetrically, the line <span
class="math inline"><em>O</em><sub><em>R</em></sub> − <em>X</em></span>
is seen by the right camera as a point and is seen as epipolar line
<span
class="math inline"><em>e</em><sub><em>L</em></sub> − <em>x</em><sub><em>L</em></sub></span>
by the left camera.</p>
<p><img src="image.png" /></p>
<p>The red lines are epipolar lines</p>
<p>When two image planes are parallel then the epipoles <span
class="math inline"><em>e</em></span> and <span
class="math inline"><em>e</em>′</span> are located at infinity. Then the
epipolar lines are parallel to <span
class="math inline"><em>x</em></span> axis of image.</p>
<p><img src="image-2.png" /></p>
<h3 id="epipolar-plane">1.4. Epipolar plane</h3>
<p><span
class="math inline"><em>X</em>, <em>O</em><sub><em>L</em></sub>, <em>O</em><sub><em>R</em></sub></span>
form a plane, called epipolar plane. The epipolar plane and all epipolar
lines intersect the epipoles regardless of where <span
class="math inline"><em>X</em></span> is located.</p>
<h3 id="epipolar-constraint">1.5. Epipolar constraint</h3>
<p>If the relative position of the two cameras is known, this leads to
two important observations:</p>
<p>Assume the projection point <span
class="math inline"><em>x</em><sub><em>L</em></sub></span>, the epipolar
line <span
class="math inline"><em>e</em><sub><em>R</em></sub> − <em>x</em><sub><em>R</em></sub></span>
and the point <span class="math inline"><em>X</em></span> projects into
the right image is known. A point <span
class="math inline"><em>x</em><sub><em>R</em></sub></span> which must
lie on this particular epipolar line.</p>
<p>This provides an epipolar constraint: the projection of <span
class="math inline"><em>X</em></span> on the right camera plane xR must
be contained in the <span
class="math inline"><em>e</em><sub><em>R</em></sub> − <em>x</em><sub><em>R</em></sub></span>
epipolar line. All points <span class="math inline"><em>X</em></span>
e.g. <span
class="math inline"><em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, <em>X</em><sub>3</sub></span>
on the <span
class="math inline"><em>O</em><sub><em>L</em></sub>–<em>X</em><sub><em>L</em></sub></span>
line will verify that constraint.</p>
<p>Epipolar constraints can also be described by the essential matrix or
the fundamental matrix between the two cameras.</p>
<h3 id="disparity-and-depth-map">1.6. Disparity and Depth map</h3>
<p><img src="image-10.png" /></p>
<p>The above diagram contains equivalent triangles. Writing their
equivalent equations will yield us following result:</p>
<p><span class="math display">$$
\text{disparity}=x-x'=\frac{Bf}{Z}
$$</span> So the depth <span class="math inline"><em>Z</em></span> would
be: <span class="math display">$$
\text{depth}=\frac{x-x'}{Bf}
$$</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline"><em>x</em></span> and <span
class="math inline"><em>x</em>′</span> are the distance between image
points and their corresponding camera center.</p></li>
<li><p><span class="math inline"><em>B</em></span> is the baseline,
distance between two camera center.</p></li>
<li><p><span class="math inline"><em>f</em></span> is the focal of both
camera (they should have the same).</p></li>
</ul>
<p>So in short, the above equation says that the depth of a point in a
scene is inversely proportional to the difference in distance of
corresponding image points and their camera centers.</p>
<h2 id="essential-matrix">2. Essential matrix</h2>
<h3 id="coordinate-representation">2.1. Coordinate representation</h3>
<p>This derivation follows the paper by Longuet-Higgins.</p>
<p>For simplicity, we assume all the cameras are
<strong>normalized</strong> and project the 3D world onto their
respective image planes. i.e <span
class="math inline"><em>K</em> = <em>K</em>′ = <em>I</em></span>.</p>
<p>Let the 3D coordinates of a point <strong>P</strong> be <span
class="math inline">(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, <em>x</em><sub>3</sub>)</span>
and <span
class="math inline">(<em>x</em>′<sub>1</sub>, <em>x</em>′<sub>2</sub>, <em>x</em>′<sub>3</sub>)</span>
relative to each cmera’s coordinate system.</p>
<p>The mapping from the coordinates of a 3D point P to the 2D image
coordinates of the point’s projection onto the image plane, according to
the pinhole camera model, is given by:</p>
<p><span class="math display">$$
\left(\begin{array}{cc}
y_1\\
y_2\\
1
\end{array}\right)
=
\frac{1}{x_3}
\left(\begin{array}{cc}
x_1\\
x_2\\
x_3
\end{array}\right)
\quad \text{and} \quad
\left(\begin{array}{cc}
y'_1\\
y'_2\\
1
\end{array}\right)
=
\frac{1}{x_3}
\left(\begin{array}{cc}
x'_1\\
x'_2\\
x'_3
\end{array}\right)
$$</span></p>
<p>More compactly as:</p>
<p><span class="math display">$$
\begin{align}
y = \frac{1}{x_3} {x} \quad \text{and} \quad y' = \frac{1}{x_3}{x}
\end{align}
$$</span></p>
<p>Where <span class="math inline"><em>y</em></span> are the image 2D
coordinate, <span class="math inline"><em>x</em></span> are real life 3D
cooordinates.</p>
<h3 id="set-up-camera-framework">2.2. Set up camera framework</h3>
<p><img src="image-3.png" /></p>
<p>Let further assume world reference coordinate is associated with the
first camera with the second camera offset by a <span
class="math inline">3 × 3</span> rotaion matrix <span
class="math inline"><em>R</em></span> and 3 dimensional translation
matrix <span class="math inline"><em>t</em></span>. This implies:</p>
<p><span class="math display">$$
\begin{align}
\vec{x'} = R({x} - t)
\end{align}
$$</span></p>
<p>So the camera matrix will be:</p>
<p><span class="math display">$$
\begin{align}
M = P[I &amp; 0] \quad \text{and} \quad M' = P'[R^T &amp; R^T-t]'
\end{align}
$$</span></p>
<p>Or (because of cameras are normalized):</p>
<p><span class="math display">$$
\begin{align}
M = [I &amp; 0] \quad \text{and} \quad M' = [R^T &amp; R^T-t]'
\end{align}
$$</span></p>
<h3 id="essential-matrix-derivation">2.3. Essential matrix
derivation</h3>
<p>Since the vector <span
class="math inline"><em>x</em> = <em>R</em>(<em>x</em>′ − <em>T</em>)</span>
and <span class="math inline"><em>t</em></span> lie in the same epipolar
line, then their cross product would produce a vector normal to the
epipolar plane:</p>
<p><span class="math display">$$
\begin {align}
{x}.[t \times (R{x}')]=0
\end {align}
$$</span></p>
<p><strong>Reminder from linear algebra:</strong> The cross product
between any two vectors a and b as a matrix-vector multiplication: <span
class="math display">$$
\begin {align}
a \times b =
\left(\begin{array}{cc}
0 &amp; -a_z &amp; a_y\\
a_z &amp; 0 &amp; -a_x \\
-a_y &amp; a_x &amp; 0
\end{array}\right)
\left(\begin{array}{cc}
b_x\\
b_y\\
b_z
\end{array}\right) =
[a_\times]b
\end {align}
$$</span></p>
<p>Combining this expression with Equation 6, we can convert the cross
product term into matrix multiplication, giving:</p>
<p><span
class="math display"><em>x</em><sup><em>T</em></sup>.[<em>t</em><sub>×</sub>](<em>R</em><em>x</em>′) = 0</span></p>
<p><span class="math display">$$
\begin {align}
x^T[t_\times]Rx'=0
\end {align}
$$</span></p>
<p>Then, the <strong>Essential Matrix</strong> is <span
class="math inline"><em>E</em> = [<em>t</em>×]<em>R</em></span> creating
a com- pact expression for the epipolar constraint: <span
class="math display">$$
\begin {align}
x^TEx'=0
\end {align}
$$</span></p>
<p>The Essential matrix is a 3 × 3 matrix that contains 5 degrees of
freedom. It has rank 2 and is singular.</p>
<h3 id="essential-matrix-mapping">2.4. Essential matrix mapping</h3>
<p>Different to a homography which maps a point to a point, an essential
matrix maps a <strong>point</strong> to a <strong>line</strong>.
Furthermore, let’s consider an epipolar line <span
class="math inline"><em>l</em></span>, with the form of <span
class="math inline"><em>a</em><em>x</em> + <em>b</em><em>y</em> + <em>c</em> = 0</span>,
or in vector form:</p>
<p><span class="math display">$$
\begin{aligned}
l=
\left(\begin{array}{c}
a\\
b\\
c
\end{array}\right) \quad \text{and} \quad x^T l=0
\end{aligned}
$$</span></p>
<p>Then it is easy to see that, from Equation 8:</p>
<p><span class="math display">$$
\begin{align}
l=Ex' \quad \text{and} \quad l'=E^Tx
\end{align}
$$</span></p>
<h3 id="essential-matrix-kernel">2.5. Essential matrix kernel</h3>
<p>Since every lines on image plane pass epipolar. So <span
class="math inline"><em>e</em><sup><em>T</em></sup><em>l</em> = 0</span>,
combine with Equation 9, we have <span
class="math inline"><em>e</em><sup><em>T</em></sup><em>l</em> = 0 = <em>e</em><sup><em>T</em></sup><em>E</em><em>x</em>′</span>.
Furthermore, <span
class="math inline"><em>e</em><sup><em>T</em></sup><em>E</em> = (<em>E</em><sup><em>T</em></sup><em>e</em>)<sup><em>T</em></sup> = <em>l</em>′<sup><em>T</em></sup></span>
thus is normal with <span class="math inline"><em>x</em>′</span>.</p>
<p>In short, essential kernel defines the epipole: <span
class="math display">$$
\begin {align}
e^TE=0 &amp; and &amp; Ee'=0
\end {align}
$$</span> (points in normalized camera coordinates)</p>
<h2 id="fundamental-matrix">3. Fundamental matrix</h2>
<h3 id="camera-matrix">3.1. Camera matrix</h3>
<p>How do you generalize to uncalibrated cameras? Recall the Equation 3:
<span class="math display">$$
\begin{align}
M = K[I &amp; 0] &amp; and &amp; M' = K'[R^T &amp; R^T-t]'
\end{align}
$$</span> First, we must definde <span
class="math inline"><em>X</em><sub><em>W</em></sub></span> is a point
from 3d world. We get two projected point on cameras:</p>
<p><span class="math display">$$
\begin {aligned}
X_0=P X_W\\
X_0'=P'X_W
\end {aligned}
$$</span></p>
<p>Say we have canonical cameras transform space by a general homography
matrix <span class="math inline"><em>H</em> = <em>I</em></span>, then we
have projections of <span
class="math inline"><em>X</em><sub><em>W</em></sub></span> to the
corresponding camera images.</p>
<p><span class="math display">$$
\begin {aligned}
x = K^-1 X_W\\
x'=K^-1'X_W'
\end {aligned}
$$</span></p>
<h3 id="fundamental-matrix-derivation">3.2. Fundamental matrix
derivation</h3>
<p>Recall that in the canonical case from Equation 7: <span
class="math display">$$
\begin {align}
x^T[t_\times]Rx'=0
\end {align}
$$</span></p>
<p>By substituting in the values of <span
class="math inline"><em>x</em></span> and <span
class="math inline"><em>x</em>′</span>, we get: <span
class="math display">$$
\begin {align}
x^T K^{-T}  [t_\times] R K'^{-1} x' =0
\end {align}
$$</span></p>
<p>Let the matrix <span
class="math inline"><em>F</em> = <em>K</em><sup>−<em>T</em></sup>[<em>t</em><sub>×</sub>]<em>R</em><em>K</em>′<sup>−1</sup></span>
as the <strong>Fundamental Matrix</strong> which acts the same to the
Essential matrix from previous but also encondes information about the
camera matrices <span class="math inline"><em>K</em></span> and <span
class="math inline"><em>K</em>′</span> and the relative translation T
and rotation R between the cameras.</p>
<p>Therefore, it is also useful in computing the epipolar lines
associated with p and p′, even when the camera matrices K, K′ and the
transformation R, T are unknown.</p>
<h3 id="propertises-of-fundamental-matrix">3.3. Propertises of
fundamental matrix</h3>
<p>Similar to the Essential matrix, we can compute the epipolar lines
<span
class="math inline"><em>l</em>′ = <em>F</em><sup><em>T</em></sup><em>x</em></span>
and <span class="math inline"><em>l</em> = <em>F</em><em>x</em>′</span>
from just the fundamental matrix and the corresponding points.</p>
<p>Fundamental matrix contains 7 degrees of freedom, while Essential
matrix’s 5 degrees of freedom.</p>
<p>If we know the fundamental matrix, then simply knowing a point in an
image gives us an easy constraint (the epipolar line) of the
corresponding point in the other image. Therefore, without knowing the
actual position of <span
class="math inline"><em>X</em><sub><em>W</em></sub></span> in 3D space,
or any of the extrinsic or intrinsic characteristics of the cameras, we
can establish a relationship between any <span
class="math inline"><em>x</em></span> and <span
class="math inline"><em>x</em>′</span>.</p>
<h2 id="the-eight-point-algorithm">4. The Eight-Point algorithm</h2>
<h3 id="formulating-a-homogeneous-linear-equation">4.1. Formulating a
homogeneous linear equation</h3>
<p>With each correspondent <span class="math inline"><em>x</em></span>
and <span class="math inline"><em>x</em>′</span></p>
<p><span class="math display">$$\begin{aligned}
x=
\left(\begin{array}{cc}
x_1\\
x_2\\
1
\end{array}\right)
\quad \text{and} \quad
x'=
\left(\begin{array}{cc}
x'_1\\
x'_2\\
1
\end{array}\right)
F =
\left(\begin{array}{cc}
f_{11} &amp; f_{12} &amp; f_{13}\\
f_{21} &amp; f_{22} &amp; f_{23}\\
f_{31} &amp; f_{32} &amp; f_{33}
\end{array}\right)
\end{aligned}$$</span></p>
<p>The constraint can be rewritten as:</p>
<p><span class="math display">$$\left(\begin{array}{cc}
x'_1x_1 &amp; x'_1x_2 &amp; x'_1 &amp;
x'_2x_1 &amp; x'_2x_2 &amp; x'_2 &amp;
x_1 &amp; x_2 &amp; 1
\end{array}\right)
\begin {aligned}
\left(\begin{array}{cc}
f_{11} \\ f_{12} \\ f_{13}\\
f_{21} \\ f_{22} \\ f_{23}\\
f_{31} \\ f_{32} \\ f_{33}
\end{array}\right)
=0
\end {aligned}$$</span></p>
<p>That is <span class="math inline"><em>f</em></span> represents the
flatten <strong>Fundamental matrix</strong> vector and this vector must
be othorgonal to vector <span
class="math inline"><strong>x̄</strong> = <em>x</em>′<em>x</em><sup><em>T</em></sup></span>.</p>
<p>Each pair of corresponding image points produces a vector <span
class="math inline"><strong>x̄</strong></span>. Given a set of 3D points
<span
class="math inline"><strong>X</strong><sub><strong>W</strong></sub></span>
corresponding to a set of vector <span
class="math inline"><strong>x̄</strong></span> and all of them must
satisfy:</p>
<p><span
class="math display"><strong>x̄</strong> ⋅ <em>f</em> = 0</span></p>
<p>Collect <span class="math inline"><em>N</em></span> vector <span
class="math inline"><strong>x̄</strong></span> as the row of matrix <span
class="math inline"><strong>X</strong></span> and: <span
class="math display"><em>X</em><em>f</em> = 0</span></p>
<p>Where <span class="math inline"><strong>X</strong></span> is a <span
class="math inline"><em>N</em> × 9</span> matrix with <span
class="math inline"><em>N</em> ≥ 8</span>.</p>
<h3 id="solving-the-equation">4.2. Solving the equation</h3>
<p>In pracitce, there are noise so solution vector f is defined only up
to an unknown scale. So it is better to use more than eight
correspondences and create a larger <span
class="math inline"><em>X</em></span>. Furthermore, <span
class="math inline"><em>X</em></span> is often rank-deficient, so we
approximate <span class="math inline"><em>f</em></span> by
<strong>Linear least squares</strong>:</p>
<p><span class="math display">$$
\begin{align}
\begin {split}
    \min_f &amp; \quad \lVert Xf\rVert \\
    \text{subject to} &amp; \quad \lVert f\rVert =1
\end{split}
\end{align}
$$</span></p>
<p>The subject is to avoid the trivial solution f.</p>
<p>The solution to this optimize problem can be found by Singular Value
Decomposition (SVD). <span class="math inline"><em>f</em></span> is the
right singular vector corresponding to the smallest singular value of
<span class="math inline"><em>X</em></span>. A reshape of this <span
class="math inline"><em>f</em></span> into <span
class="math inline">3 × 3</span> matrix give result called as <span
class="math inline"><strong>F</strong><sub><strong>e</strong><strong>s</strong><strong>t</strong></sub></span>.</p>
<h3 id="enforcing-the-internal-constraint">4.3. Enforcing the internal
constraint</h3>
<p>An important property of the fundamental matrix is that it is
singular, in fact of rank 2. Furthermore, the left and right null spaces
(<span class="math inline"><em>e</em></span> and <span
class="math inline"><em>e</em>′</span>) of <span
class="math inline"><em>F</em></span> are generated by the vectors
representing the two epipoles in the images i.e <span
class="math inline"><em>d</em><em>i</em><em>m</em><em>N</em><em>u</em><em>l</em><em>l</em>(<em>F</em>) = 1</span>.
However, often, dealing with noisy image gives the result <span
class="math inline"><strong>F</strong><sub><strong>e</strong><strong>s</strong><strong>t</strong></sub></span>
from Equation 14 usually does not have rank 2.</p>
<p>We find a best rank-2 matrix approximation of F by the mean of: <span
class="math display">$$
\begin{align}
    \begin{split}
    \min_F &amp; \quad \lVert F_{est} - F\rVert \\
    \text{subject to} &amp; \quad \det F =0
    \end {split}
\end{align}
$$</span> The constrain is to make <span
class="math inline"><em>F</em></span> is singular.</p>
<p>This problem is solved again by SVD, where <span
class="math inline"><em>F</em> = <em>U</em><em>Σ</em><em>V</em><sup><em>T</em></sup></span>
then the best rank-2 approximation is found by:</p>
<p><span class="math display">$$
F = U
\begin{bmatrix}
\Sigma_1 &amp; 0 &amp; 0 \\
0 &amp; \Sigma_2 &amp; 0 \\
0 &amp; 0 &amp; 0
\end{bmatrix}
V^T
$$</span></p>
<h3 id="normalized-algorithm">4.4. Normalized algorithm</h3>
<h4 id="problems">4.4.1. Problems</h4>
<p>The problem of the standard algorithm is that <span
class="math inline"><em>X</em></span> is often ill-conditioned for SVD.
For SVD to work properly, <span class="math inline"><em>X</em></span>
shuld have one singular value equal or near to zero, with the rest are
non zero.</p>
<p>However, correspondences coordinate <span
class="math inline">(<em>x</em>, <em>y</em>, 1)</span> will often have
extremely large values in the first and second compared to the third
<span class="math inline">(<em>x̄</em> = (1920, 1080, 1))</span> due to
large pixel range of mordern digital camera.</p>
<p>Furthermore, if the image points used to construct <span
class="math inline"><em>X</em></span> lie in a relatively small region
of the image <span class="math inline">((700, 700) ± 100)</span>, then
<span class="math inline"><em>x</em></span> and <span
class="math inline"><em>x</em>′</span> are relatively similar, resulting
in <span class="math inline"><em>X</em></span> has one ery large
singular value, with the rest relatively small.</p>
<h4 id="solution">4.4.2. Solution</h4>
<p>To solve this, map each coordinate system of two images independently
into a new system satisfying two conditions:</p>
<ul>
<li><p>The origin of the new system should be at the centroid (center of
gravity) of the image points. This is accomplished by translating
original origin to new one.</p></li>
<li><p>After the translation, the coordinates have to be uniformed so
that the mean of distance from each points to the origin equals <span
class="math inline">$\sqrt 2$</span>. This can e done by the scaling
factor for each respective image</p></li>
</ul>
<p><span class="math display">$$
\displaystyle \sqrt \frac{2N}{\displaystyle \sum_{i=1}^N \lVert x_i-\mu
\rVert^2}
$$</span></p>
<p>Afterwards, a distinct coordinate transformation for each of the two
images. We obtain a new homogeneous image: <span class="math display">$$
\begin {align*}
\bar x = Tx \\
\bar x'= T'x'
\end{align*}
$$</span></p>
<p>This normalization is only dependent on the image points which are
used in a single image and is, in general, distinct from normalized
image coordinates produced by a normalized camera.</p>
<p>Note that we overload the notations because of the obivious
relations.</p>
<p>The epipolar constraint based on the fundamental matrix can now be
rewritten as:</p>
<p><span
class="math display"><em>x</em><sup><em>T</em></sup><em>F</em><em>x</em>′ = <em>x̄</em>′<sup><em>T</em></sup><em>T</em>′<sup>−<em>T</em></sup><em>F</em><em>T</em><sup>−1</sup><em>T</em><em>x̄</em> = <em>x̄</em>′<sup><em>T</em></sup><em>F̄</em><em>x̄</em> = 0</span></p>
<p>Where <span
class="math inline"><em>F̄</em> = <em>T</em>′<sup>−<em>T</em></sup><em>F</em><em>T</em><sup>−1</sup><em>T</em></span>.</p>
<p>This means that it is possible to use the normalized homogeneous
image coordinates, <span class="math inline"><em>x̄</em></span> and <span
class="math inline"><em>x̄</em>′</span>, to estimate the transformed
fundamental matrix <span class="math inline"><em>F̄</em></span> using the
basic eight-point algorithm described above.</p>
<p>The solution <span class="math inline"><em>F̄</em></span> is now more
well-defined from the homogeneous equation <span
class="math inline"><em>X̄</em><em>F̄</em></span> than <span
class="math inline"><em>F</em></span> is relative to <span
class="math inline"><em>X</em></span>. Once <span
class="math inline"><em>F̄</em></span> has been determined, we can
de-normalized to give <span class="math inline"><em>F</em></span> by:
<span
class="math display"><em>F</em> = <em>T</em>′<sup><em>T</em></sup><em>F̄</em><em>T</em></span></p>
<h2 id="image-rectification">5. Image Rectification</h2>
<p>Recall that when two image planes are parallel then the epipoles
<span class="math inline"><em>e</em></span> and <span
class="math inline"><em>e</em>′</span> are located at infinity and the
epipolar lines are parallel to the <span
class="math inline"><em>x</em></span> axis of image. We can assume that
the two cameras has the same intrinsic <span
class="math inline"><em>K</em></span> and there is no rotation between
them <span class="math inline"><em>R</em> = <em>I</em></span>.
Furthermore, we assume there is only a translation along the <span
class="math inline"><em>x</em></span> axis <span
class="math inline"><em>T</em> = (<em>T</em><sub><em>x</em></sub>, 0, 0)</span>.
Then the essential matrix would be:</p>
<p><span class="math display">$$\begin{align}
E_{rect}=[T_\times] R =
T_\times
\left(
\begin{array}{cc}
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; -1 \\
0 &amp; 1 &amp; 0
\end{array}\right)
\end {align}$$</span></p>
<p>Once <span class="math inline"><em>E</em></span> is known, we can
find the directions of the epipolar line <span
class="math inline"><em>l</em></span> associated with point <span
class="math inline"><em>x</em>′</span> in the second image plane:</p>
<p><span class="math display">$$\begin{align*}
l = Ex'=
\left(\begin{array}{cc}
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; -T_\times \\
0 &amp; T_\times &amp; 0
\end{array}\right)
\left(\begin{array}{cc}
x'_1 \\
x'_2 \\
1
\end{array}\right)
=
\left(\begin{array}{cc}
0 \\
-T_\times\\
T_\times x'_2
\end{array}\right)
\end {align*}$$</span></p>
<p>We can see that epipolar line <span
class="math inline"><em>l</em></span> horizontal, parallel to the <span
class="math inline"><em>x</em></span> axis. As is the direction of <span
class="math inline"><em>l</em></span>, which is computed in the same
manner.</p>
<p><img src="image-5.png" /></p>
<p>Furthermore, if we use the epipolar constraint <span
class="math inline"><em>x</em><sup><em>T</em></sup><em>E</em><em>x</em>′ = 0</span>,
then we arrive the fact that <span
class="math inline"><em>x</em><sub>2</sub> = <em>x</em>′<sub>2</sub></span>,
which is very handy in indentifying correspondences. Therefore
<strong>rectification</strong> makes any two given images become
parallel, becomes useful then discerning the relationship between
corresponding points in images.</p>
<h1 id="iiii.-solution">IIII. Solution</h1>
<p>Most of the algorithm used in our codes are described above.</p>
<p><img src="flow-chart.drawio.png" /></p>
<h2 id="calibration.py">3.1. calibration.py</h2>
<p>In this file, we compute the feature points, then estimate the
fundamental and essential matrix from them. Furthermore, we find the
correct camera pose (Rotation and Translation) from the <span
class="math inline"><em>E</em></span> matrix.</p>
<h3
id="def-draw_keypoints_and_matchimg1-matlike-img2-matlike-nfeatures-int-500---tuplendarray-ndarray-matlike">3.1.1.
def draw_keypoints_and_match(img1: MatLike, img2: MatLike, nfeatures:
int = 500) -&gt; Tuple[NDArray, NDArray, MatLike]: …</h3>
<p>Finding keypoints using ORB feature detection and descriptors in the
image and find best matches using brute force based matcher.</p>
<h3
id="def-compute_fundamental_matrixkp1_list-ndarray-kp2_list-ndarray---ndarray">3.1.2.
def compute_Fundamental_matrix(kp1_list: NDArray, kp2_list: NDArray)
-&gt; NDArray: …</h3>
<p>Calculate the <span class="math inline"><em>F</em></span> matrix from
a set of 8 points using SVD. Furthermore, the rank of <span
class="math inline"><em>F</em></span> matrix is reduced from 3 to 2 to
make the epilines converge.</p>
<p>More details at section <a href="#42-solving-the-equation">8 points
algorithm</a></p>
<h3
id="def-ransac_f_matkp1_list-ndarray-kp2_list-ndarray-max_inliers20-threshold0.05-max_iter1000---ndarray">3.1.3.
def RANSAC_F_mat(kp1_list: NDArray, kp2_list: NDArray, max_inliers=20,
threshold=0.05, max_iter=1000) -&gt; NDArray: …</h3>
<p>Shortlist the best F matrix using RANSAC based on the number of
inliers.</p>
<p>RANSAC randomize 8 feature points to compute <span
class="math inline"><em>F</em></span> and calculate errors in estimating
the points. Then we classify inliers with predefined threshold. Best
<span class="math inline"><em>F</em></span> is one with the most
inliers.</p>
<h3
id="def-compute_essential_matrixf_mat-ndarray-k1-ndarray-k2-ndarray---ndarray">3.1.4.
def compute_Essential_matrix(F_mat: NDArray, K1: NDArray, K2: NDArray)
-&gt; NDArray: …</h3>
<p>Calculation of the Essential matrix with <span
class="math inline"><em>F</em></span>, <span
class="math inline"><em>K</em><sub>1</sub></span> and <span
class="math inline"><em>K</em><sub>2</sub></span> camera intrinsic.</p>
<p>More details at section <a
href="#2-essential-matrix">Essential-matrix</a></p>
<h3
id="def-drawlinesimg1src-matlike-img2src-matlike-lines-pts1src-pts2src-random_seed0---tuplematlike-matlike">3.1.5.
def drawlines(img1src: MatLike, img2src: MatLike, lines, pts1src,
pts2src, random_seed=0) -&gt; tuple[MatLike, MatLike]: …</h3>
<p>Visualize the epilines on both the images</p>
<h2 id="correspondence.py">3.2. correspondence.py</h2>
<h3 id="def-sum_of_squared_diffpixel_vals_1-pixel_vals_2">3.2.1. def
sum_of_squared_diff(pixel_vals_1, pixel_vals_2):</h3>
<p>We use SSD formular:</p>
<p><span
class="math display">SSD = ∑<sub><em>i</em>, <em>j</em></sub>(<em>I</em><sub>1</sub>(<em>i</em>, <em>j</em>) − <em>I</em><sub>2</sub>(<em>i</em>, <em>j</em>))<sup>2</sup></span></p>
<h3
id="def-block_comparisony-x-block_left-right_array-block_size-x_search_block_size-y_search_block_size">3.2.2.
def block_comparison(y, x, block_left, right_array, block_size,
x_search_block_size, y_search_block_size):</h3>
<p>Block comparison function to find minimum SSD match. Two image block
are correspodence if their SSD is the minimum.</p>
<h3 id="def-ssd_correspondenceimg1-img2">3.2.3. def
ssd_correspondence(img1, img2):</h3>
<p>Correspondence applied on the whole image to compute the disparity
map and finally disparity map is scaled by min max scaling.</p>
<h2 id="depth.py">3.3. depth.py</h2>
<h3 id="def-disparity_to_depthbaseline-f-img">3.3.1. def
disparity_to_depth(baseline, f, img):</h3>
<p><span class="math display">$$\text{depth} =
\frac{Bf}{x-x'}$$</span></p>
<p>More details at section <a href="#16-disparity-and-depth-map">Depth
Map</a></p>
<h1 id="iiii.-evaluation">IIII. Evaluation</h1>
<h2 id="the-results-achieved">4.1. The results achieved</h2>
<h2 id="compare-with-the-results-of-the-group-on-the-same-topic">4.2.
Compare with the results of the group on the same topic</h2>
<h2 id="previous-studies">4.3. Previous studies</h2>
<h3 id="calculating-correspondences-sgm">4.3.1. Calculating
correspondences (SGM)</h3>
<p>We used Sum of Squared Differences (SSD) to calculate the
coressponding block. Howvever, this function inherently assume there is
no sudden depth variations and occlusions. Furthermore, this assume
continuous or functionally variable disparities within the correlation
window. Thus often produce very noisy results, jumping and gap
pixels.</p>
<p><img src="image-15.png" /></p>
<p>One portion of the scene may obscure other portions in either or both
of the two cameras. Ex: <span
class="math inline"><em>B</em> − <em>C</em> − <em>s</em><sub>1</sub></span>.</p>
<p>The Semi-Global Matching (SGM) implements Mutual Information cost
function. The pixelwise cost and the smoothness constraints are
expressed by defining the energy E(D) that depends on the disparity
image D:</p>
<p><span
class="math display"><em>E</em>(<em>D</em>) = ∑<sub><em>p</em></sub>(<em>C</em>(<em>p</em>, <em>D</em><sub><em>p</em></sub>) + ∑<sub><em>q</em> ∈ <em>N</em><sub><em>p</em></sub></sub><em>P</em><sub>1</sub><em>T</em>[|<em>D</em><sub><em>p</em></sub> − <em>D</em><sub><em>q</em></sub>| = 1] + ∑<sub><em>q</em> ∈ <em>N</em><sub><em>p</em></sub></sub><em>P</em><sub>2</sub><em>T</em>[|<em>D</em><sub><em>p</em></sub> − <em>D</em><sub><em>q</em></sub>| &gt; 1])</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline"><em>D</em></span> represents the
disparity map.</p></li>
<li><p><span
class="math inline"><em>C</em>(<em>p</em>, <em>D</em><sub><em>p</em></sub>)</span>
is the data term that measures the fidelity of the disparity map <span
class="math inline"><em>D</em></span> at pixel <span
class="math inline"><em>p</em></span>.</p></li>
<li><p><span class="math inline"><em>N</em><sub><em>p</em></sub></span>
denotes the neighborhood of pixel <span
class="math inline"><em>p</em></span>.</p></li>
<li><p><span class="math inline"><em>T</em></span> is an indicator
function.</p></li>
<li><p>First term is the similarity cost, the sum of all pixel matching
costs for the disparities of <span
class="math inline"><em>D</em></span>.</p></li>
<li><p><span class="math inline"><em>P</em><sub>1</sub></span> penalty
for small disparity changes (1 pixel).</p></li>
<li><p><span class="math inline"><em>P</em><sub>2</sub></span> penalty
is constant and larger for all larger disparity changes.</p></li>
</ul>
<p>Using a lower penalty for small changes permits an adaptation to
slanted or curved surfaces. The constant penalty for all larger changes
(i.e. independent of their size) preserves discontinuities. While SSD is
too simple to capture these characters.</p>
<p>Its module is available at <em>cv2.StereoBM_create</em></p>
<h3 id="disaprity-post-filtering-wsl">4.3.2. Disaprity post Filtering
(WSL)</h3>
<p>Since Weighted Least Squares (WSL) - based Fast Global Image
smoothing mathemetical background as well as its algorithm is outside of
our scope. We will not include it here.</p>
<p>Its objective is to enhance the quality of the input low-resolution
depth map by increasing its spatial resolution. Low-resolution depth map
and its associated high-resolution color image are used as inputs.</p>
<p>Its module is available at
<em>cv2.ximgproc.createDisparityWLSFilter</em></p>
<h3 id="deep-learning-based-approach">4.3.3. Deep Learning Based
Approach</h3>
<p>Deep learning</p>
<h1 id="references">References</h1>
<p>https://web.stanford.edu/class/cs231a/course_notes/03-epipolar-geometry.pdf
https://cmsc426.github.io/sfm/#essential
https://en.wikipedia.org/wiki/Epipolar_geometry
https://en.wikipedia.org/wiki/Essential_matrix
https://en.wikipedia.org/wiki/Fundamental_matrix_(computer_vision)</p>
<p>For 4. Evaluation</p>
<p>https://core.ac.uk/download/pdf/11134866.pdf
https://engineering.purdue.edu/kak/Tutorials/SemiGlobalMatching.pdf</p>
<p>WSL Filter:
https://publish.illinois.edu/visual-modeling-and-analytics/files/2014/10/FGS-TIP.pdf</p>
